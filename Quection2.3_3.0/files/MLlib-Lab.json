{"paragraphs":[{"text":"%md \n# Estimator, Transformer, and Param","dateUpdated":"2018-12-18T20:58:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Estimator, Transformer, and Param</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1545166554464_-168053535","id":"20161129-012735_1377582462","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8572","user":"anonymous","dateFinished":"2018-12-18T20:58:02+0000","dateStarted":"2018-12-18T20:58:02+0000"},{"text":"%pyspark\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.classification import LogisticRegression\n\n# Prepare training data from a list of (label, features) tuples.\ntraining = spark.createDataFrame([\n    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n\n# Create a LogisticRegression instance. This instance is an Estimator.\nlr = LogisticRegression(maxIter=10, regParam=0.01)\n# Print out the parameters, documentation, and any default values.\nprint \"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\"\n\n# Learn a LogisticRegression model. This uses the parameters stored in lr.\nmodel1 = lr.fit(training)\n\n# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n# we can view the parameters it used during fit().\n# This prints the parameter (name: value) pairs, where names are unique IDs for this\n# LogisticRegression instance.\nprint \"Model 1 was fit using parameters: \"\nprint model1.extractParamMap()\n\n# We may alternatively specify parameters using a Python dictionary as a paramMap\nparamMap = {lr.maxIter: 20}\nparamMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\nparamMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # Specify multiple Params.\n\n# You can combine paramMaps, which are python dictionaries.\nparamMap2 = {lr.probabilityCol: \"myProbability\"}  # Change output column name\nparamMapCombined = paramMap.copy()\nparamMapCombined.update(paramMap2)\n\n# Now learn a new model using the paramMapCombined parameters.\n# paramMapCombined overrides all parameters set earlier via lr.set* methods.\nmodel2 = lr.fit(training, paramMapCombined)\nprint \"Model 2 was fit using parameters: \"\nprint model2.extractParamMap()\n\n# Prepare test data\ntest = spark.createDataFrame([\n    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n\n# Make predictions on test data using the Transformer.transform() method.\n# LogisticRegression.transform will only use the 'features' column.\n# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n# 'probability' column since we renamed the lr.probabilityCol parameter previously.\nprediction = model2.transform(test)\nselected = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\")\nfor row in selected.collect():\n    print row","dateUpdated":"2018-12-18T20:58:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"LogisticRegression parameters:\naggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\nfeaturesCol: features column name. (default: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label)\nmaxIter: max number of iterations (>= 0). (default: 100, current: 10)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nregParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n\nModel 1 was fit using parameters: \n{}\nModel 2 was fit using parameters: \n{}\nRow(features=DenseVector([-1.0, 1.5, 1.3]), label=1.0, myProbability=DenseVector([0.0571, 0.9429]), prediction=1.0)\nRow(features=DenseVector([3.0, 2.0, -0.1]), label=0.0, myProbability=DenseVector([0.9239, 0.0761]), prediction=0.0)\nRow(features=DenseVector([0.0, 2.2, -1.5]), label=1.0, myProbability=DenseVector([0.1097, 0.8903]), prediction=1.0)\n"}]},"apps":[],"jobName":"paragraph_1545166554465_-168438284","id":"20161129-012748_52787394","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8573","user":"anonymous","dateFinished":"2018-12-18T20:58:05+0000","dateStarted":"2018-12-18T20:58:02+0000"},{"text":"%md\n\n#Extracting, transforming and selecting features\n\n## CountVectorizer\n","dateUpdated":"2018-12-18T20:58:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>#Extracting, transforming and selecting features</p>\n<h2>CountVectorizer</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1545166554465_-168438284","id":"20161129-013007_1388002982","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8574","user":"anonymous","dateFinished":"2018-12-18T20:58:02+0000","dateStarted":"2018-12-18T20:58:02+0000"},{"text":"%pyspark\n\nfrom pyspark.ml.feature import CountVectorizer\n\n# Input data: Each row is a bag of words with a ID.\ndf = spark.createDataFrame([\n    (0, \"a b c\".split(\" \")),\n    (1, \"a b b c a\".split(\" \"))\n], [\"id\", \"words\"])\n\n# fit a CountVectorizerModel from the corpus.\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\nmodel = cv.fit(df)\nresult = model.transform(df)\nresult.show()","dateUpdated":"2018-12-18T20:58:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+---------------+--------------------+\n| id|          words|            features|\n+---+---------------+--------------------+\n|  0|      [a, b, c]|(3,[0,1,2],[1.0,1...|\n|  1|[a, b, b, c, a]|(3,[0,1,2],[2.0,2...|\n+---+---------------+--------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1545166554465_-168438284","id":"20161129-013020_1764358722","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8575","user":"anonymous","dateFinished":"2018-12-18T20:58:06+0000","dateStarted":"2018-12-18T20:58:02+0000"},{"text":"%md\n\n## Feature Transformers : Tokenizer","dateUpdated":"2018-12-18T20:58:02+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545166554465_-168438284","id":"20161129-013707_1456126627","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8576","user":"anonymous","dateFinished":"2018-12-18T20:58:02+0000","dateStarted":"2018-12-18T20:58:02+0000"},{"text":"%pyspark \n\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\n\nsentenceDataFrame = spark.createDataFrame([\n    (0, \"Hi I heard about Spark\"),\n    (1, \"I wish Java could use case classes\"),\n    (2, \"Logistic,regression,models,are,neat\")\n], [\"label\", \"sentence\"])\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsDataFrame = tokenizer.transform(sentenceDataFrame)\nfor words_label in wordsDataFrame.select(\"words\", \"label\").take(3):\n    print(words_label)\nregexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n# alternatively, pattern=\"\\\\w+\", gaps(False)","dateUpdated":"2018-12-18T20:58:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Row(words=[u'hi', u'i', u'heard', u'about', u'spark'], label=0)\nRow(words=[u'i', u'wish', u'java', u'could', u'use', u'case', u'classes'], label=1)\nRow(words=[u'logistic,regression,models,are,neat'], label=2)\n"}]},"apps":[],"jobName":"paragraph_1545166554466_-167284037","id":"20161129-013723_581839226","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8577","user":"anonymous","dateFinished":"2018-12-18T20:58:06+0000","dateStarted":"2018-12-18T20:58:05+0000"},{"text":"%md \n## Feature Selectors: VectorSlicer","dateUpdated":"2018-12-18T20:58:02+0000","config":{"colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545166554466_-167284037","id":"20161129-013842_1827572139","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8578","user":"anonymous","dateFinished":"2018-12-18T20:58:02+0000","dateStarted":"2018-12-18T20:58:02+0000"},{"text":"%pyspark\n\nfrom pyspark.ml.feature import VectorSlicer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import Row\n\ndf = spark.createDataFrame([\n    Row(userFeatures=Vectors.sparse(3, {0: -2.0, 1: 2.3}),),\n    Row(userFeatures=Vectors.dense([-2.0, 2.3, 0.0]),)])\n\nslicer = VectorSlicer(inputCol=\"userFeatures\", outputCol=\"features\", indices=[1])\n\noutput = slicer.transform(df)\n\noutput.select(\"userFeatures\", \"features\").show()","dateUpdated":"2018-12-18T20:58:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-------------+\n|        userFeatures|     features|\n+--------------------+-------------+\n|(3,[0,1],[-2.0,2.3])|(1,[0],[2.3])|\n|      [-2.0,2.3,0.0]|        [2.3]|\n+--------------------+-------------+\n\n"}]},"apps":[],"jobName":"paragraph_1545166554466_-167284037","id":"20161129-013858_179930977","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8579","user":"anonymous","dateFinished":"2018-12-18T20:58:06+0000","dateStarted":"2018-12-18T20:58:06+0000"},{"text":"%md\n\n#Classification","dateUpdated":"2018-12-18T20:58:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545166554466_-167284037","id":"20161129-005933_456191674","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8580","user":"anonymous","dateFinished":"2018-12-18T20:58:02+0000","dateStarted":"2018-12-18T20:58:02+0000"},{"text":"%md\n\n## Logistic regression\nLogistic regression is a popular method to predict a binary response. It is a special case of Generalized Linear models that predicts the probability of the outcome.","dateUpdated":"2018-12-18T20:58:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Logistic regression</h2>\n<p>Logistic regression is a popular method to predict a binary response. It is a special case of Generalized Linear models that predicts the probability of the outcome.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545166554467_-167668786","id":"20161129-010017_1506647346","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8581","user":"anonymous","dateFinished":"2018-12-18T20:58:03+0000","dateStarted":"2018-12-18T20:58:03+0000"},{"text":"%pyspark \n\n#The following example shows how to train a logistic regression model with elastic net regularization\n\nfrom pyspark.ml.classification import LogisticRegression\n\n# Load training data\ntraining = spark.read.format(\"libsvm\").load(\"/data/mllib/sample_libsvm_data.txt\")\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n\n# Fit the model\nlrModel = lr.fit(training)\n\n# Print the coefficients and intercept for logistic regression\nprint(\"Coefficients: \" + str(lrModel.coefficients))\nprint(\"Intercept: \" + str(lrModel.intercept))\n","dateUpdated":"2018-12-18T20:58:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.35398352419e-05,-9.10273850559e-05,-0.000194674305469,-0.000203006424735,-3.14761833149e-05,-6.84297760266e-05,1.58836268982e-05,1.40234970914e-05,0.00035432047525,0.000114432728982,0.000100167123837,0.00060141093038,0.000284024817912,-0.000115410847365,0.000385996886313,0.000635019557424,-0.000115064123846,-0.00015271865865,0.000280493380899,0.000607011747119,-0.000200845966325,-0.000142107557929,0.000273901034116,0.00027730456245,-9.83802702727e-05,-0.000380852244352,-0.000253151980086,0.000277477147708,-0.000244361976392,-0.00153947446876,-0.000230733284113])\nIntercept: 0.224563159613\n"}]},"apps":[],"jobName":"paragraph_1545166554467_-167668786","id":"20161129-010106_1160729799","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8582","user":"anonymous","dateFinished":"2018-12-18T20:58:08+0000","dateStarted":"2018-12-18T20:58:07+0000"},{"text":"%md \n#Decision tree classifier\n","dateUpdated":"2018-12-18T20:58:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>#Decision tree classifier</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545166554467_-167668786","id":"20161129-010330_366107409","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8583","user":"anonymous","dateFinished":"2018-12-18T20:58:03+0000","dateStarted":"2018-12-18T20:58:03+0000"},{"text":"%pyspark\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Load the data stored in LIBSVM format as a DataFrame.\ndata = spark.read.format(\"libsvm\").load(\"/data/mllib/sample_libsvm_data.txt\")\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)","dateUpdated":"2018-12-18T20:58:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+------------+--------------------+\n|prediction|indexedLabel|            features|\n+----------+------------+--------------------+\n|       1.0|         1.0|(692,[98,99,100,1...|\n|       1.0|         1.0|(692,[124,125,126...|\n|       1.0|         1.0|(692,[124,125,126...|\n|       1.0|         1.0|(692,[124,125,126...|\n|       1.0|         1.0|(692,[126,127,128...|\n+----------+------------+--------------------+\nonly showing top 5 rows\n\nTest Error = 0 \nDecisionTreeClassificationModel (uid=DecisionTreeClassifier_4a9f99cf2da3cd444fcb) of depth 2 with 5 nodes\n"}]},"apps":[],"jobName":"paragraph_1545166554467_-167668786","id":"20161129-010551_944142688","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8584","user":"anonymous","dateFinished":"2018-12-18T20:58:12+0000","dateStarted":"2018-12-18T20:58:07+0000"},{"text":"%md\n# Regression\n\n## Random forest regression","dateUpdated":"2018-12-18T20:58:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Regression</h1>\n<h2>Random forest regression</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1545166554467_-167668786","id":"20161129-010635_572224039","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8585","user":"anonymous","dateFinished":"2018-12-18T20:58:03+0000","dateStarted":"2018-12-18T20:58:03+0000"},{"text":"%pyspark\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Load and parse the data file, converting it to a DataFrame.\ndata = spark.read.format(\"libsvm\").load(\"/data/mllib/sample_libsvm_data.txt\")\n\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n\n# Chain indexer and forest in a Pipeline\npipeline = Pipeline(stages=[featureIndexer, rf])\n\n# Train model.  This also runs the indexer.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = RegressionEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n\nrfModel = model.stages[1]\nprint(rfModel)  # summary only","dateUpdated":"2018-12-18T20:58:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-----+--------------------+\n|prediction|label|            features|\n+----------+-----+--------------------+\n|       0.0|  0.0|(692,[98,99,100,1...|\n|      0.15|  0.0|(692,[100,101,102...|\n|      0.05|  0.0|(692,[122,123,148...|\n|       0.0|  0.0|(692,[123,124,125...|\n|       0.0|  0.0|(692,[123,124,125...|\n+----------+-----+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 0.0790569\nRandomForestRegressionModel (uid=rfr_8c8795c5e5e3) with 20 trees\n"}]},"apps":[],"jobName":"paragraph_1545166554467_-167668786","id":"20161129-011540_1646360459","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8586","user":"anonymous","dateFinished":"2018-12-18T20:58:14+0000","dateStarted":"2018-12-18T20:58:08+0000"},{"text":"%md\n\n# Clustering\n\n## K-means","dateUpdated":"2018-12-18T20:58:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Clustering</h1>\n<h2>K-means</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1545166554468_-169592530","id":"20161129-011600_996178656","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8587","user":"anonymous","dateFinished":"2018-12-18T20:58:03+0000","dateStarted":"2018-12-18T20:58:03+0000"},{"text":"%pyspark\n\nfrom pyspark.ml.clustering import KMeans\n\n# Loads data.\ndataset = spark.read.format(\"libsvm\").load(\"/data/mllib/sample_kmeans_data.txt\")\n\n# Trains a k-means model.\nkmeans = KMeans().setK(2).setSeed(1)\nmodel = kmeans.fit(dataset)\n\n# Evaluate clustering by computing Within Set Sum of Squared Errors.\nwssse = model.computeCost(dataset)\nprint(\"Within Set Sum of Squared Errors = \" + str(wssse))\n\n# Shows the result.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)\n    ","dateUpdated":"2018-12-18T20:58:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Within Set Sum of Squared Errors = 0.12\nCluster Centers: \n[ 0.1  0.1  0.1]\n[ 9.1  9.1  9.1]\n"}]},"apps":[],"jobName":"paragraph_1545166554468_-169592530","id":"20161129-014411_927055504","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8588","user":"anonymous","dateFinished":"2018-12-18T20:58:15+0000","dateStarted":"2018-12-18T20:58:13+0000"},{"text":"%md\n\n#Cross-Validation\n","dateUpdated":"2018-12-18T20:58:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>#Cross-Validation</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545166554468_-169592530","id":"20161129-014829_2105425566","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8589","user":"anonymous","dateFinished":"2018-12-18T20:58:03+0000","dateStarted":"2018-12-18T20:58:03+0000"},{"text":"%pyspark\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n# Prepare training documents, which are labeled.\ntraining = spark.createDataFrame([\n    (0, \"a b c d e spark\", 1.0),\n    (1, \"b d\", 0.0),\n    (2, \"spark f g h\", 1.0),\n    (3, \"hadoop mapreduce\", 0.0),\n    (4, \"b spark who\", 1.0),\n    (5, \"g d a y\", 0.0),\n    (6, \"spark fly\", 1.0),\n    (7, \"was mapreduce\", 0.0),\n    (8, \"e spark program\", 1.0),\n    (9, \"a e c l\", 0.0),\n    (10, \"spark compile\", 1.0),\n    (11, \"hadoop software\", 0.0)\n], [\"id\", \"text\", \"label\"])\n\n# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n\n# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n# This will allow us to jointly choose parameters for all Pipeline stages.\n# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n# We use a ParamGridBuilder to construct a grid of parameters to search over.\n# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\nparamGrid = ParamGridBuilder() \\\n    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n    .addGrid(lr.regParam, [0.1, 0.01]) \\\n    .build()\n\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=2)  # use 3+ folds in practice\n\n# Run cross-validation, and choose the best set of parameters.\ncvModel = crossval.fit(training)\n\n# Prepare test documents, which are unlabeled.\ntest = spark.createDataFrame([\n    (4, \"spark i j k\"),\n    (5, \"l m n\"),\n    (6, \"mapreduce spark\"),\n    (7, \"apache hadoop\")\n], [\"id\", \"text\"])\n\n# Make predictions on test documents. cvModel uses the best model found (lrModel).\nprediction = cvModel.transform(test)\nselected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\nfor row in selected.collect():\n    print(row)","dateUpdated":"2018-12-18T20:58:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Row(id=4, text=u'spark i j k', probability=DenseVector([0.2661, 0.7339]), prediction=1.0)\nRow(id=5, text=u'l m n', probability=DenseVector([0.9209, 0.0791]), prediction=0.0)\nRow(id=6, text=u'mapreduce spark', probability=DenseVector([0.4429, 0.5571]), prediction=1.0)\nRow(id=7, text=u'apache hadoop', probability=DenseVector([0.8584, 0.1416]), prediction=0.0)\n"}]},"apps":[],"jobName":"paragraph_1545166554468_-169592530","id":"20161129-015223_1699705288","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8590","user":"anonymous","dateFinished":"2018-12-18T20:58:28+0000","dateStarted":"2018-12-18T20:58:14+0000"},{"text":"","dateUpdated":"2018-12-18T20:58:03+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545166554468_-169592530","id":"20161129-015302_1115287316","dateCreated":"2018-12-18T20:55:54+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8591","user":"anonymous"}],"name":"MLlib-Lab-Session","id":"2DY5A3SN4","angularObjects":{"2DZTP6D2D:shared_process":[],"2DY24CTP9:shared_process":[],"2DXP8V47F:shared_process":[],"2E1APAZ5R:shared_process":[],"2E1V6JJ91:shared_process":[],"2DYHP1KDT:shared_process":[],"2DX5YXM35:shared_process":[],"2DXG7A5UV:shared_process":[],"2DYBXK8WC:shared_process":[],"2DXMAHN63:shared_process":[],"2DXRJAN8F:shared_process":[],"2DYYZFDWW:shared_process":[],"2DX52NXAG:shared_process":[],"2DXR8TEX8:shared_process":[],"2DYAJQ68Z:shared_process":[],"2DZSHPWQ8:shared_process":[],"2DZGBMEXZ:shared_process":[],"2E1GP6NWC:shared_process":[],"2DYTN3YCM:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}